{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "YOLOv5 Training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3Si8WMHyG-PN",
        "ShKsc2g2DxKL"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPb0OrDbjB21gvpf796wEHz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaleNurse/YOLOv5_Trainer/blob/main/YOLOv5_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GEZSbEoEiZ6"
      },
      "source": [
        "# Setup environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju4WizM829oI"
      },
      "source": [
        "Connect Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce8PGyZj286w",
        "outputId": "5b12e7d3-4a03-496e-a6ab-586407cfb8f7"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL1hZIdnC7Ey"
      },
      "source": [
        "Update environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuVh0_N7-T5z",
        "outputId": "970f891b-b022-47ca-f289-6a68fcd9d2a4"
      },
      "source": [
        "! apt update\r\n",
        "! apt upgrade\r\n",
        "! apt install libcudnn7 libcublas-dev libcublas10 libcudnn7 libcudnn7-dev libnccl-dev libnccl2 --allow-change-held-packages\r\n",
        "! echo \"Segment complete\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [Waiting for headers] [1 InRelease 14.2 kB/88.7 kB 16%] [Connecting to cloud\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [1 InRelease 88.7 kB/88.7 kB 100%] [Connecting to cloud.r-project.org] [Wait\u001b[0m\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r                                                                               \rHit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [3 InRelease 85.1 kB/88.7 kB 96%] [Connecting to cloud.r-project.org] [Conne\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 242 kB] [3 InRelease 85.1 kB/88.7 kB 96%] [Connecting to c\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 242 kB] [Connecting to cloud.r-project.org] [Connecting to\u001b[0m\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "\u001b[33m\r0% [2 InRelease gpgv 242 kB] [6 InRelease 22.9 kB/74.6 kB 31%] [Connecting to c\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 242 kB] [Connecting to cloud.r-project.org] [Waiting for h\u001b[0m\r                                                                               \rHit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,921 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,387 kB]\n",
            "Ign:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:12 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,729 kB]\n",
            "Get:13 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [885 kB]\n",
            "Ign:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:16 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,352 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,157 kB]\n",
            "Get:20 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [44.8 kB]\n",
            "Ign:22 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:22 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [552 kB]\n",
            "Fetched 11.3 MB in 3s (3,563 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "20 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following packages have been kept back:\n",
            "  libcublas-dev libcublas10 libcudnn7 libcudnn7-dev libnccl-dev libnccl2\n",
            "The following packages will be upgraded:\n",
            "  cuda-compat-10-1 libaudit-common libaudit1 libc-bin libldap-2.4-2\n",
            "  libldap-common libp11-kit0 libsasl2-2 libsasl2-modules-db linux-libc-dev\n",
            "  r-cran-cachem r-cran-dbplyr r-cran-nlme tar\n",
            "14 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n",
            "Need to get 10.6 MB of archives.\n",
            "After this operation, 201 kB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-cachem amd64 1.0.3-1cran1.1804.0 [57.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 tar amd64 1.29b-2ubuntu0.2 [234 kB]\n",
            "Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-dbplyr all 2.1.0-1cran1.1804.0 [761 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libc-bin amd64 2.27-3ubuntu1.4 [643 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  cuda-compat-10-1 418.181.07-1 [5,253 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaudit-common all 1:2.8.2-1ubuntu1.1 [4,068 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaudit1 amd64 1:2.8.2-1ubuntu1.1 [38.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libp11-kit0 amd64 0.23.9-2ubuntu0.1 [187 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsasl2-modules-db amd64 2.1.27~101-g0780600+dfsg-3ubuntu2.3 [15.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsasl2-2 amd64 2.1.27~101-g0780600+dfsg-3ubuntu2.3 [49.2 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-common all 2.4.45+dfsg-1ubuntu1.9 [16.6 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-2.4-2 amd64 2.4.45+dfsg-1ubuntu1.9 [155 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 linux-libc-dev amd64 4.15.0-135.139 [990 kB]\n",
            "Get:14 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ r-cran-nlme 3.1.152-1.1804.0 [2,230 kB]\n",
            "Fetched 10.6 MB in 0s (29.6 MB/s)\n",
            "(Reading database ... 146442 files and directories currently installed.)\n",
            "Preparing to unpack .../tar_1.29b-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking tar (1.29b-2ubuntu0.2) over (1.29b-2ubuntu0.1) ...\n",
            "Setting up tar (1.29b-2ubuntu0.2) ...\n",
            "update-alternatives: warning: forcing reinstallation of alternative /usr/sbin/rmt-tar because link group rmt is broken\n",
            "(Reading database ... 146442 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-bin_2.27-3ubuntu1.4_amd64.deb ...\n",
            "Unpacking libc-bin (2.27-3ubuntu1.4) over (2.27-3ubuntu1.3) ...\n",
            "Setting up libc-bin (2.27-3ubuntu1.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "(Reading database ... 146442 files and directories currently installed.)\n",
            "Preparing to unpack .../libaudit-common_1%3a2.8.2-1ubuntu1.1_all.deb ...\n",
            "Unpacking libaudit-common (1:2.8.2-1ubuntu1.1) over (1:2.8.2-1ubuntu1) ...\n",
            "Setting up libaudit-common (1:2.8.2-1ubuntu1.1) ...\n",
            "(Reading database ... 146442 files and directories currently installed.)\n",
            "Preparing to unpack .../libaudit1_1%3a2.8.2-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libaudit1:amd64 (1:2.8.2-1ubuntu1.1) over (1:2.8.2-1ubuntu1) ...\n",
            "Setting up libaudit1:amd64 (1:2.8.2-1ubuntu1.1) ...\n",
            "(Reading database ... 146442 files and directories currently installed.)\n",
            "Preparing to unpack .../libp11-kit0_0.23.9-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libp11-kit0:amd64 (0.23.9-2ubuntu0.1) over (0.23.9-2) ...\n",
            "Setting up libp11-kit0:amd64 (0.23.9-2ubuntu0.1) ...\n",
            "(Reading database ... 146442 files and directories currently installed.)\n",
            "Preparing to unpack .../0-cuda-compat-10-1_418.181.07-1_amd64.deb ...\n",
            "Unpacking cuda-compat-10-1 (418.181.07-1) over (418.165.02-1) ...\n",
            "Preparing to unpack .../1-libsasl2-modules-db_2.1.27~101-g0780600+dfsg-3ubuntu2.3_amd64.deb ...\n",
            "Unpacking libsasl2-modules-db:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.3) over (2.1.27~101-g0780600+dfsg-3ubuntu2.1) ...\n",
            "Preparing to unpack .../2-libsasl2-2_2.1.27~101-g0780600+dfsg-3ubuntu2.3_amd64.deb ...\n",
            "Unpacking libsasl2-2:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.3) over (2.1.27~101-g0780600+dfsg-3ubuntu2.1) ...\n",
            "Preparing to unpack .../3-libldap-common_2.4.45+dfsg-1ubuntu1.9_all.deb ...\n",
            "Unpacking libldap-common (2.4.45+dfsg-1ubuntu1.9) over (2.4.45+dfsg-1ubuntu1.8) ...\n",
            "Preparing to unpack .../4-libldap-2.4-2_2.4.45+dfsg-1ubuntu1.9_amd64.deb ...\n",
            "Unpacking libldap-2.4-2:amd64 (2.4.45+dfsg-1ubuntu1.9) over (2.4.45+dfsg-1ubuntu1.8) ...\n",
            "Preparing to unpack .../5-linux-libc-dev_4.15.0-135.139_amd64.deb ...\n",
            "Unpacking linux-libc-dev:amd64 (4.15.0-135.139) over (4.15.0-130.134) ...\n",
            "Preparing to unpack .../6-r-cran-cachem_1.0.3-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-cachem (1.0.3-1cran1.1804.0) over (1.0.1-1cran1.1804.0) ...\n",
            "Preparing to unpack .../7-r-cran-dbplyr_2.1.0-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-dbplyr (2.1.0-1cran1.1804.0) over (2.0.0-1cran1.1804.0) ...\n",
            "Preparing to unpack .../8-r-cran-nlme_3.1.152-1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-nlme (3.1.152-1.1804.0) over (3.1.151-1.1804.0) ...\n",
            "Setting up libldap-common (2.4.45+dfsg-1ubuntu1.9) ...\n",
            "Setting up cuda-compat-10-1 (418.181.07-1) ...\n",
            "Setting up r-cran-nlme (3.1.152-1.1804.0) ...\n",
            "Setting up libsasl2-modules-db:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.3) ...\n",
            "Setting up linux-libc-dev:amd64 (4.15.0-135.139) ...\n",
            "Setting up libsasl2-2:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.3) ...\n",
            "Setting up r-cran-cachem (1.0.3-1cran1.1804.0) ...\n",
            "Setting up libldap-2.4-2:amd64 (2.4.45+dfsg-1ubuntu1.9) ...\n",
            "Setting up r-cran-dbplyr (2.1.0-1cran1.1804.0) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following held packages will be changed:\n",
            "  libcublas-dev libcublas10 libcudnn7 libnccl-dev libnccl2\n",
            "The following packages will be upgraded:\n",
            "  libcublas-dev libcublas10 libcudnn7 libcudnn7-dev libnccl-dev libnccl2\n",
            "6 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
            "Need to get 520 MB of archives.\n",
            "After this operation, 157 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcublas10 10.2.3.254-1 [43.1 MB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcublas-dev 10.2.3.254-1 [42.4 MB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libcudnn7-dev 7.6.5.32-1+cuda10.2 [165 MB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libcudnn7 7.6.5.32-1+cuda10.2 [189 MB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libnccl-dev 2.8.4-1+cuda11.2 [39.3 MB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libnccl2 2.8.4-1+cuda11.2 [40.6 MB]\n",
            "Fetched 520 MB in 10s (52.7 MB/s)\n",
            "(Reading database ... 146442 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libcublas10_10.2.3.254-1_amd64.deb ...\n",
            "Unpacking libcublas10 (10.2.3.254-1) over (10.2.1.243-1) ...\n",
            "Preparing to unpack .../1-libcublas-dev_10.2.3.254-1_amd64.deb ...\n",
            "Unpacking libcublas-dev (10.2.3.254-1) over (10.2.1.243-1) ...\n",
            "Preparing to unpack .../2-libcudnn7-dev_7.6.5.32-1+cuda10.2_amd64.deb ...\n",
            "update-alternatives: removing manually selected alternative - switching libcudnn to auto mode\n",
            "Unpacking libcudnn7-dev (7.6.5.32-1+cuda10.2) over (7.6.5.32-1+cuda10.1) ...\n",
            "Preparing to unpack .../3-libcudnn7_7.6.5.32-1+cuda10.2_amd64.deb ...\n",
            "Unpacking libcudnn7 (7.6.5.32-1+cuda10.2) over (7.6.5.32-1+cuda10.1) ...\n",
            "Preparing to unpack .../4-libnccl-dev_2.8.4-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libnccl-dev (2.8.4-1+cuda11.2) over (2.8.3-1+cuda10.1) ...\n",
            "Preparing to unpack .../5-libnccl2_2.8.4-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libnccl2 (2.8.4-1+cuda11.2) over (2.8.3-1+cuda10.1) ...\n",
            "Setting up libcudnn7 (7.6.5.32-1+cuda10.2) ...\n",
            "Setting up libnccl2 (2.8.4-1+cuda11.2) ...\n",
            "Setting up libcudnn7-dev (7.6.5.32-1+cuda10.2) ...\n",
            "update-alternatives: using /usr/include/x86_64-linux-gnu/cudnn_v7.h to provide /usr/include/cudnn.h (libcudnn) in auto mode\n",
            "Setting up libcublas10 (10.2.3.254-1) ...\n",
            "Setting up libcublas-dev (10.2.3.254-1) ...\n",
            "Setting up libnccl-dev (2.8.4-1+cuda11.2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Segment complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3w02tPW3L-E"
      },
      "source": [
        "Check GPU and Driver Version (req >=460)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtMJ8Ooh3NvG",
        "outputId": "a9442fb2-89ae-4d04-d821-0faf3e3af87d"
      },
      "source": [
        "gpu_info = !nvidia-smi\r\n",
        "gpu_info = '\\n'.join(gpu_info)\r\n",
        "if gpu_info.find('failed') >= 0:\r\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\r\n",
        "  print('and then re-execute this cell.')\r\n",
        "else:\r\n",
        "  print(gpu_info)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb  8 18:33:50 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjHc-JA22jwt"
      },
      "source": [
        "Install YOLOv5 and depends"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oSE4BClo1hjj",
        "outputId": "bf4d0bc3-2df5-4bdb-bae9-328ad2866212"
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\r\n",
        "!pip install imgaug==0.2.5  # deal w/ dependency issue\r\n",
        "!pip install -U -r yolov5/requirements.txt  # install dependencies\r\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html # update torch to work with newer drivers\r\n",
        "!pip install wandb\r\n",
        "!echo \"Segment complete RESTART\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 4696, done.\u001b[K\n",
            "remote: Total 4696 (delta 0), reused 0 (delta 0), pack-reused 4696\u001b[K\n",
            "Receiving objects: 100% (4696/4696), 7.48 MiB | 9.36 MiB/s, done.\n",
            "Resolving deltas: 100% (3209/3209), done.\n",
            "Collecting imgaug==0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/60/a06a48d85a7e9062f5870347a3e3e953da30b37928d43b380c949bca458a/imgaug-0.2.5.tar.gz (562kB)\n",
            "\u001b[K     |████████████████████████████████| 563kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.5) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.5) (0.16.2)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.5) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.5) (1.15.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5) (2.5)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5) (7.0.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.5) (2.4.7)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug==0.2.5) (4.4.2)\n",
            "Building wheels for collected packages: imgaug\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.5-cp36-none-any.whl size=561439 sha256=559e1e4f71398116eca7612d2ed57d37967919c6d0795bf0b2c2ba914ca6e237\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/48/c8/ca3345e8582a078de94243996e148377ef66fdb845557bae0b\n",
            "Successfully built imgaug\n",
            "Installing collected packages: imgaug\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "Successfully installed imgaug-0.2.5\n",
            "Requirement already up-to-date: Cython in /usr/local/lib/python3.6/dist-packages (from -r yolov5/requirements.txt (line 4)) (0.29.21)\n",
            "Collecting matplotlib>=3.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/03/b7b30fa81cb687d1178e085d0f01111ceaea3bf81f9330c937fb6f6c8ca0/matplotlib-3.3.4-cp36-cp36m-manylinux1_x86_64.whl (11.5MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5MB 10.6MB/s \n",
            "\u001b[?25hRequirement already up-to-date: numpy>=1.18.5 in /usr/local/lib/python3.6/dist-packages (from -r yolov5/requirements.txt (line 6)) (1.19.5)\n",
            "Collecting opencv-python>=4.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/20/4d78eb1ce337efd609ade8ebe0c82260cd47dd73f8c57dcfe4814c6a3b59/opencv_python-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (50.4MB)\n",
            "\u001b[K     |████████████████████████████████| 50.4MB 119kB/s \n",
            "\u001b[?25hCollecting Pillow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/c0/442d9d87e0da00bf856ef6dd4916f84a2d710b5f1a367d42d7f3c4e99a6c/Pillow-8.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 40.0MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/5b/bc0b5ab38247bba158504a410112b6c03f153c652734ece1849749e5f518/PyYAML-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (640kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 38.3MB/s \n",
            "\u001b[?25hCollecting scipy>=1.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/89/63171228d5ced148f5ced50305c89e8576ffc695a90b58fe5bb602b910c2/scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9MB 1.4MB/s \n",
            "\u001b[?25hRequirement already up-to-date: tensorboard>=2.2 in /usr/local/lib/python3.6/dist-packages (from -r yolov5/requirements.txt (line 11)) (2.4.1)\n",
            "Collecting torch>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/4f/acf48b3a18a8f9223c6616647f0a011a5713a985336088d7c76f3a211374/torch-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 22kB/s \n",
            "\u001b[?25hCollecting torchvision>=0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/f1/d1d9b2be9f50e840accfa180ec2fb759dd2504f2b3a12a232398d5fa00ae/torchvision-0.8.2-cp36-cp36m-manylinux1_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 23.8MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.41.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/02/8f8880a4fd6625461833abcf679d4c12a44c76f9925f92bf212bb6cefaad/tqdm-4.56.0-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.8MB/s \n",
            "\u001b[?25hRequirement already up-to-date: seaborn>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from -r yolov5/requirements.txt (line 20)) (0.11.1)\n",
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (from -r yolov5/requirements.txt (line 21)) (1.1.5)\n",
            "Collecting thop\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/8b/22ce44e1c71558161a8bd54471123cc796589c7ebbfc15a7e8932e522f83/thop-0.0.31.post2005241907-py3-none-any.whl\n",
            "Requirement already up-to-date: pycocotools>=2.0 in /usr/local/lib/python3.6/dist-packages (from -r yolov5/requirements.txt (line 30)) (2.0.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.2.2->-r yolov5/requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.2.2->-r yolov5/requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.2.2->-r yolov5/requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.2.2->-r yolov5/requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (1.25.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (3.3.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (53.0.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r yolov5/requirements.txt (line 12)) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->-r yolov5/requirements.txt (line 12)) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r yolov5/requirements.txt (line 21)) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (4.7)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2->-r yolov5/requirements.txt (line 11)) (3.4.0)\n",
            "Installing collected packages: Pillow, matplotlib, opencv-python, PyYAML, scipy, torch, torchvision, tqdm, thop\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: torchvision 0.8.1+cu101\n",
            "    Uninstalling torchvision-0.8.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.8.1+cu101\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed Pillow-8.1.0 PyYAML-5.4.1 matplotlib-3.3.4 opencv-python-4.5.1.48 scipy-1.5.4 thop-0.0.31.post2005241907 torch-1.7.1 torchvision-0.8.2 tqdm-4.56.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (735.4MB)\n",
            "\u001b[K     |████████████████████████████████| 735.4MB 25kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (0.8)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.2+cu101) (8.1.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.7.1\n",
            "    Uninstalling torch-1.7.1:\n",
            "      Successfully uninstalled torch-1.7.1\n",
            "  Found existing installation: torchvision 0.8.2\n",
            "    Uninstalling torchvision-0.8.2:\n",
            "      Successfully uninstalled torchvision-0.8.2\n",
            "Successfully installed torch-1.7.1+cu101 torchvision-0.8.2+cu101\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/85/941042d55708fe30f95d799b5380ea5601ea6c50ab3cf06c0b2dad02129c/wandb-0.10.17-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/08/b2/ef713e0e67f6e7ec7d59aea3ee78d05b39c15930057e724cc6d362a8c3bb/configparser-5.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/5c/018bf9a5c24343a664deaea70e61f33f53bb1bd3caf193110f827bfd07e2/sentry_sdk-0.19.5-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 42.4MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/cb/ec98155c501b68dcb11314c7992cd3df6dce193fd763084338a117967d53/GitPython-3.1.12-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 25.9MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.1)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.12.4)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.0->wandb) (53.0.0)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6490 sha256=18b90c7144b0836893b5d97ebdc19544f0d200af30968eb3eeee8c500f20e993\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8785 sha256=ca6ec924c8619ca519be45f372d87918d19109afdff2115c0c03262a29ad4f07\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: configparser, sentry-sdk, subprocess32, smmap, gitdb, GitPython, docker-pycreds, pathtools, shortuuid, wandb\n",
            "Successfully installed GitPython-3.1.12 configparser-5.0.1 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-0.19.5 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.17\n",
            "Segment complete RESTART\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upjb_W286Dk4"
      },
      "source": [
        "Update YOLOv5 if, for some reason, it's already cloned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyMobjlO537P"
      },
      "source": [
        "%cd /content/yolov5\r\n",
        "!git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTwubH1i2es0"
      },
      "source": [
        "Check Torch environment (ensure no errors thrown)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwlF4OXt1t_M",
        "outputId": "e48867d5-e5c6-49d9-d73b-a3c5c795408a"
      },
      "source": [
        "%cd /content/yolov5\r\n",
        "import torch\r\n",
        "from IPython.display import Image  # for displaying images\r\n",
        "from utils.google_utils import gdrive_download  # for downloading models/datasets\r\n",
        "\r\n",
        "print('torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5\n",
            "torch 1.7.1+cu101 _CudaDeviceProperties(name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhXTjwObBUjz"
      },
      "source": [
        "Customize iPython writefile so we can write variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAzD_ZQNBKlF",
        "outputId": "d7453d81-7e67-4af5-90fd-a88474e46164"
      },
      "source": [
        "from IPython.core.magic import register_line_cell_magic\r\n",
        "\r\n",
        "@register_line_cell_magic\r\n",
        "def writetemplate(line, cell):\r\n",
        "    with open(line, 'w') as f:\r\n",
        "        f.write(cell.format(**globals()))\r\n",
        "!echo \"Segment complete\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Segment complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5SXLdJXEntT"
      },
      "source": [
        "# Setup training model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBkC2LMXeSsE"
      },
      "source": [
        "Define training parameters and make dependant paths/strings\r\n",
        "\r\n",
        "**Known good configs**\r\n",
        "\r\n",
        "yolov5x batch 18 img 640 epoch 250 w/ deepstack mod\r\n",
        "\r\n",
        "yolov5l batch 20 img 736 epoch ??? w/ deepstack mod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvESB6SWQGa8",
        "outputId": "121b8c68-17a8-4c92-807b-659c38261372"
      },
      "source": [
        "# Set variables above separator. Don't delete the quote marks!\r\n",
        "\r\n",
        "# Known good configs\r\n",
        "# yolov5x batch 18 img 640 epoch 250 w/ deepstack mod\r\n",
        "# yolov5l batch 20 img 736 epoch ??? w/ deepstack mod\r\n",
        "\r\n",
        "model_size = 'l'        # trailing end of the training model desired (s, m, l, or x)\r\n",
        "check = 'y'              # Use a checkpoint in the traing? ('y' if yes)\r\n",
        "img_size = '736'        # to what resolution to scale training images (e.g. 640) must be mult of 32\r\n",
        "batch_size = '20'\r\n",
        "epochs = '300'\r\n",
        "comment = 'aug3_ds'      # something to distringuish the run name, if needed (e.g. aug_dsx)\r\n",
        "test_img_size = '736'   # to what resolution to scale testing images (e.g. 960)  must be mult of 32\r\n",
        "test_conf = '0.5'       # confidence threshold for inference testing (e.g. 0.5)\r\n",
        "\r\n",
        "##########################################################################################\r\n",
        "\r\n",
        "import os.path, shutil, os\r\n",
        "from os import path\r\n",
        "\r\n",
        "i = 1\r\n",
        "j = 1\r\n",
        "\r\n",
        "#make base model path\r\n",
        "model_yaml = '/content/yolov5/models/yolov5' + model_size + '.yaml'\r\n",
        "\r\n",
        "if not comment: \r\n",
        "  #make run name if no comment entered\r\n",
        "  run_name = '5' + model_size + '_i' + img_size + '_b' + batch_size\r\n",
        "else:\r\n",
        "  #make run name if comment exists\r\n",
        "  run_name = '5' + model_size + '_i' + img_size + '_b' + batch_size + '_' + comment\r\n",
        "\r\n",
        "#if directory exists with current run name, start incrementing run name by 1 until unique\r\n",
        "if path.exists(path.join('/content/drive/MyDrive/colab_out/', run_name)):                 \r\n",
        "  while path.exists(path.join('/content/drive/MyDrive/colab_out/', run_name + str(i))):\r\n",
        "    i += 1\r\n",
        "  run_name = run_name + str(i)\r\n",
        "\r\n",
        "#set path to run\r\n",
        "run_path = '/content/drive/MyDrive/colab_out/' + run_name\r\n",
        "\r\n",
        "exp_dir = '/content/drive/MyDrive/colab_out/exp'\r\n",
        "\r\n",
        "#find next available exp directory if \"exp\" already exists\r\n",
        "if path.exists('/content/drive/MyDrive/colab_out/exp'):                 \r\n",
        "  while path.exists(path.join('/content/drive/MyDrive/colab_out/exp' + str(j))):\r\n",
        "    j += 1\r\n",
        "  exp_dir = exp_dir + str(j)\r\n",
        "\r\n",
        "#make string for inference testing output\r\n",
        "exp_img = path.join(exp_dir, '*.jpg')\r\n",
        "\r\n",
        "#set path to weights\r\n",
        "weights = run_path + '/weights/best.pt'\r\n",
        "\r\n",
        "if check == 'y':\r\n",
        "  chkpnt = 'yolov5' + model_size + '.pt'\r\n",
        "  chkpnt_tmp = path.join('/content/drive/MyDrive/v3_weights', chkpnt)\r\n",
        "  shutil.copy(chkpnt_tmp, '/content/yolov5/')\r\n",
        "else:\r\n",
        "  chkpnt = ''\r\n",
        "\r\n",
        "!echo \"This run is called $run_name and the exp folder will be $exp_dir\""
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This run is called 5l_i736_b20_aug3_ds and the exp folder will be /content/drive/MyDrive/colab_out/exp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ALP9BrRiC0VS",
        "outputId": "03821a8a-d7d9-4c70-d501-943339dbcfff"
      },
      "source": [
        "import os.path, shutil, os\r\n",
        "from os import path\r\n",
        "\r\n",
        "shutil.copy(chkpnt_tmp, '/content/yolov5/')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/yolov5/yolov5l.pt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkyZbsSYBNED"
      },
      "source": [
        "Define number of classes based on YAML from Roboflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K6CjIyo7KzA",
        "outputId": "12e67b79-4db7-4688-dd73-dbbeb4b19415"
      },
      "source": [
        "%cd /content/drive/MyDrive/my-dataset\r\n",
        "import yaml\r\n",
        "with open(\"data.yaml\", 'r') as stream:\r\n",
        "    num_classes = str(yaml.safe_load(stream)['nc'])\r\n",
        "!echo \"Segment complete\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/my-dataset\n",
            "Segment complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPrj-wxwoSE7"
      },
      "source": [
        "Modify Roboflow YAML with correct directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNJOamLhrJV3",
        "outputId": "98cdaa41-4e77-4334-ff1e-ece6e52719f6"
      },
      "source": [
        "%cd /content/drive/MyDrive/my-dataset\r\n",
        "!sed 's/\\.\\./\\/content\\/drive\\/MyDrive\\/my-dataset/' data.yaml >temp.yaml\r\n",
        "!mv temp.yaml data.yaml\r\n",
        "!echo \"Segment complete\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/my-dataset\n",
            "Segment complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9DYYFMsBmIm"
      },
      "source": [
        "Output desired model size (adjust as necessary)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc9s-rnbBt3x",
        "outputId": "83ea3b02-20f2-4958-e79b-bc7f8a2785a4"
      },
      "source": [
        "%cat $model_yaml"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# parameters\n",
            "nc: 80  # number of classes\n",
            "depth_multiple: 1.0  # model depth multiple\n",
            "width_multiple: 1.0  # layer channel multiple\n",
            "\n",
            "# anchors\n",
            "anchors:\n",
            "  - [10,13, 16,30, 33,23]  # P3/8\n",
            "  - [30,61, 62,45, 59,119]  # P4/16\n",
            "  - [116,90, 156,198, 373,326]  # P5/32\n",
            "\n",
            "# YOLOv5 backbone\n",
            "backbone:\n",
            "  # [from, number, module, args]\n",
            "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
            "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
            "   [-1, 3, C3, [128]],\n",
            "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
            "   [-1, 9, C3, [256]],\n",
            "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
            "   [-1, 9, C3, [512]],\n",
            "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
            "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
            "   [-1, 3, C3, [1024, False]],  # 9\n",
            "  ]\n",
            "\n",
            "# YOLOv5 head\n",
            "head:\n",
            "  [[-1, 1, Conv, [512, 1, 1]],\n",
            "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
            "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
            "   [-1, 3, C3, [512, False]],  # 13\n",
            "\n",
            "   [-1, 1, Conv, [256, 1, 1]],\n",
            "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
            "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
            "   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n",
            "\n",
            "   [-1, 1, Conv, [256, 3, 2]],\n",
            "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
            "   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n",
            "\n",
            "   [-1, 1, Conv, [512, 3, 2]],\n",
            "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
            "   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n",
            "\n",
            "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
            "  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBZ6JdOGB3Ko"
      },
      "source": [
        "Create custom model using exact class size.\r\n",
        "\r\n",
        "Copy the cat output above starting at depth_multiple and replace the same below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffA7EbTCCDvX"
      },
      "source": [
        "%%writetemplate /content/yolov5/models/custom_yolov5.yaml\r\n",
        "\r\n",
        "# parameters\r\n",
        "nc: {num_classes}  # number of classes\r\n",
        "depth_multiple: 1.0  # model depth multiple\r\n",
        "width_multiple: 1.0  # layer channel multiple\r\n",
        "\r\n",
        "# anchors\r\n",
        "anchors:\r\n",
        "  - [10,13, 16,30, 33,23]  # P3/8\r\n",
        "  - [30,61, 62,45, 59,119]  # P4/16\r\n",
        "  - [116,90, 156,198, 373,326]  # P5/32\r\n",
        "\r\n",
        "# YOLOv5 backbone\r\n",
        "backbone:\r\n",
        "  # [from, number, module, args]\r\n",
        "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\r\n",
        "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\r\n",
        "   [-1, 3, C3, [128]],\r\n",
        "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\r\n",
        "   [-1, 9, C3, [256]],\r\n",
        "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\r\n",
        "   [-1, 9, C3, [512]],\r\n",
        "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\r\n",
        "   [-1, 1, SPP, [1024, [5, 9, 13]]],\r\n",
        "   [-1, 3, C3, [1024, False]],  # 9\r\n",
        "  ]\r\n",
        "\r\n",
        "# YOLOv5 head\r\n",
        "head:\r\n",
        "  [[-1, 1, Conv, [512, 1, 1]],\r\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\r\n",
        "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\r\n",
        "   [-1, 3, C3, [512, False]],  # 13\r\n",
        "\r\n",
        "   [-1, 1, Conv, [256, 1, 1]],\r\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\r\n",
        "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\r\n",
        "   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\r\n",
        "\r\n",
        "   [-1, 1, Conv, [256, 3, 2]],\r\n",
        "   [[-1, 14], 1, Concat, [1]],  # cat head P4\r\n",
        "   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\r\n",
        "\r\n",
        "   [-1, 1, Conv, [512, 3, 2]],\r\n",
        "   [[-1, 10], 1, Concat, [1]],  # cat head P5\r\n",
        "   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\r\n",
        "\r\n",
        "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\r\n",
        "  ]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbrOUV1E-_9Z"
      },
      "source": [
        "Adjust model if training for DeepStack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0UFly1Z-Sro",
        "outputId": "9aad34f4-96cb-44c4-b406-83ab2041712c"
      },
      "source": [
        "%cd /content/yolov5/models/\r\n",
        "!sed 's/C3/BottleneckCSP/' custom_yolov5.yaml >temp.yaml\r\n",
        "!mv temp.yaml custom_yolov5.yaml\r\n",
        "!echo \"Segment complete\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5/models\n",
            "Segment complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1U3o5Y-g5JF"
      },
      "source": [
        "Make sure the custom model looks OK (and exists)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnW-FsbCg2k2",
        "outputId": "a353792e-0b06-4ab8-8d01-6c444e435eb8"
      },
      "source": [
        "!cat /content/yolov5/models/custom_yolov5.yaml"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# parameters\n",
            "nc: 11  # number of classes\n",
            "depth_multiple: 1.0  # model depth multiple\n",
            "width_multiple: 1.0  # layer channel multiple\n",
            "\n",
            "# anchors\n",
            "anchors:\n",
            "  - [10,13, 16,30, 33,23]  # P3/8\n",
            "  - [30,61, 62,45, 59,119]  # P4/16\n",
            "  - [116,90, 156,198, 373,326]  # P5/32\n",
            "\n",
            "# YOLOv5 backbone\n",
            "backbone:\n",
            "  # [from, number, module, args]\n",
            "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
            "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
            "   [-1, 3, BottleneckCSP, [128]],\n",
            "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
            "   [-1, 9, BottleneckCSP, [256]],\n",
            "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
            "   [-1, 9, BottleneckCSP, [512]],\n",
            "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
            "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
            "   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
            "  ]\n",
            "\n",
            "# YOLOv5 head\n",
            "head:\n",
            "  [[-1, 1, Conv, [512, 1, 1]],\n",
            "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
            "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
            "   [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
            "\n",
            "   [-1, 1, Conv, [256, 1, 1]],\n",
            "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
            "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
            "   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n",
            "\n",
            "   [-1, 1, Conv, [256, 3, 2]],\n",
            "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
            "   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n",
            "\n",
            "   [-1, 1, Conv, [512, 3, 2]],\n",
            "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
            "   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n",
            "\n",
            "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
            "  ]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmKAILtNFC0E"
      },
      "source": [
        "# Train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ECBD4mxFF1U",
        "outputId": "618a856d-8c71-40c1-fb39-0d773467bd4a"
      },
      "source": [
        "%%time\r\n",
        "%cd /content/yolov5/\r\n",
        "!python train.py --batch $batch_size --img-size $img_size --epochs $epochs --data '/content/drive/MyDrive/my-dataset/data.yaml' --cfg ./models/custom_yolov5.yaml --weights '$chkpnt' --name $run_name  --project /content/drive/MyDrive/colab_out/ --cache-images"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 v4.0-69-ga5359f6 torch 1.7.1+cu101 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\n",
            "\n",
            "Namespace(adam=False, batch_size=20, bucket='', cache_images=True, cfg='./models/custom_yolov5.yaml', data='/content/drive/MyDrive/my-dataset/data.yaml', device='', epochs=300, evolve=False, exist_ok=False, global_rank=-1, hyp='data/hyp.scratch.yaml', image_weights=False, img_size=[736, 736], linear_lr=False, local_rank=-1, log_artifacts=False, log_imgs=16, multi_scale=False, name='5l_i736_b20_aug3_ds', noautoanchor=False, nosave=False, notest=False, project='/content/drive/MyDrive/colab_out/', quad=False, rect=False, resume=False, save_dir='/content/drive/MyDrive/colab_out/5l_i736_b20_aug3_ds', single_cls=False, sync_bn=False, total_batch_size=20, weights='yolov5l.pt', workers=8, world_size=1)\n",
            "Start Tensorboard with \"tensorboard --logdir /content/drive/MyDrive/colab_out/\", view at http://localhost:6006/\n",
            "2021-02-08 19:17:11.316324: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      7040  models.common.Focus                     [3, 64, 3]                    \n",
            "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  2                -1  1    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n",
            "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  4                -1  1   1627904  models.common.BottleneckCSP             [256, 256, 9]                 \n",
            "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  6                -1  1   6499840  models.common.BottleneckCSP             [512, 512, 9]                 \n",
            "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
            "  8                -1  1   2624512  models.common.SPP                       [1024, 1024, [5, 9, 13]]      \n",
            "  9                -1  1  10234880  models.common.BottleneckCSP             [1024, 1024, 3, False]        \n",
            " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1   2823680  models.common.BottleneckCSP             [1024, 512, 3, False]         \n",
            " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1    707328  models.common.BottleneckCSP             [512, 256, 3, False]          \n",
            " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1   2561536  models.common.BottleneckCSP             [512, 512, 3, False]          \n",
            " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1  10234880  models.common.BottleneckCSP             [1024, 1024, 3, False]        \n",
            " 24      [17, 20, 23]  1     86160  models.yolo.Detect                      [11, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
            "Model Summary: 499 layers, 47447184 parameters, 47447184 gradients, 116.1 GFLOPS\n",
            "\n",
            "Transferred 650/658 items from yolov5l.pt\n",
            "Scaled weight_decay = 0.00046875\n",
            "Optimizer groups: 110 .bias, 118 conv.weight, 107 other\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmalenurse\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "2021-02-08 19:17:20.908408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m5l_i736_b20_aug3_ds\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/malenurse/colab_out\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/malenurse/colab_out/runs/11hvbuv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/yolov5/wandb/run-20210208_191719-11hvbuv5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/content/drive/MyDrive/my-dataset/train/labels.cache' for images and labels... 861 found, 0 missing, 0 empty, 0 corrupted: 100%|██████████| 861/861 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.9GB): 100%|██████████| 861/861 [00:06<00:00, 132.40it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/drive/MyDrive/my-dataset/valid/labels.cache' for images and labels... 123 found, 0 missing, 0 empty, 0 corrupted: 100%|██████████| 123/123 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB): 100%|██████████| 123/123 [00:01<00:00, 92.13it/s]\n",
            "Plotting labels... \n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.69, Best Possible Recall (BPR) = 1.0000\n",
            "Image sizes 736 train, 736 test\n",
            "Using 2 dataloader workers\n",
            "Logging results to /content/drive/MyDrive/colab_out/5l_i736_b20_aug3_ds\n",
            "Starting training for 300 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     0/299     8.49G    0.1011   0.06083   0.06654    0.2285         5       736: 100%|██████████| 44/44 [01:04<00:00,  1.47s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]\n",
            "                 all         123         517      0.0122       0.037      0.0112     0.00288\n",
            "Images sizes do not match. This will causes images to be display incorrectly in the UI.\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     1/299     15.4G   0.08114   0.07035   0.06074    0.2122        12       736: 100%|██████████| 44/44 [00:58<00:00,  1.34s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 4/4 [00:02<00:00,  1.42it/s]\n",
            "                 all         123         517       0.235       0.234       0.214       0.119\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     2/299     15.4G   0.06375   0.06078   0.05088    0.1754         5       736: 100%|██████████| 44/44 [00:59<00:00,  1.34s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 4/4 [00:02<00:00,  1.38it/s]\n",
            "                 all         123         517       0.587       0.368       0.376       0.182\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     3/299     15.4G    0.0531   0.04645   0.03985    0.1394         5       736: 100%|██████████| 44/44 [00:59<00:00,  1.34s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 4/4 [00:02<00:00,  1.38it/s]\n",
            "                 all         123         517       0.225       0.621       0.447       0.177\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     4/299     15.4G   0.04904   0.03841   0.03243    0.1199         2       736: 100%|██████████| 44/44 [00:58<00:00,  1.34s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]\n",
            "                 all         123         517       0.461       0.666       0.555       0.267\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     5/299     15.4G   0.05719   0.03601   0.02801    0.1212        13       736: 100%|██████████| 44/44 [00:57<00:00,  1.30s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 4/4 [00:02<00:00,  1.43it/s]\n",
            "                 all         123         517       0.459       0.811       0.688       0.358\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     6/299     15.4G   0.05451   0.03174   0.02434    0.1106         3       736: 100%|██████████| 44/44 [00:56<00:00,  1.29s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 4/4 [00:02<00:00,  1.43it/s]\n",
            "                 all         123         517       0.619       0.739       0.726       0.396\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     7/299     15.4G   0.05222   0.03242   0.01982    0.1045        11       736: 100%|██████████| 44/44 [00:56<00:00,  1.29s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 4/4 [00:02<00:00,  1.44it/s]\n",
            "                 all         123         517       0.599       0.752       0.753       0.413\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     8/299     15.4G   0.04933   0.03127   0.01671   0.09731        11       736: 100%|██████████| 44/44 [00:56<00:00,  1.29s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100%|██████████| 4/4 [00:02<00:00,  1.46it/s]\n",
            "                 all         123         517       0.688       0.804       0.832       0.457\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\n",
            "     9/299     15.4G   0.04722   0.02891   0.01387      0.09       117       736:  86%|████████▋ | 38/44 [00:50<00:07,  1.30s/it]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v693HyaeVJ2"
      },
      "source": [
        "# Set variables if environment lost in a runtime disconnection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTVVcjgcelOU"
      },
      "source": [
        "Make sure to rerun environment setup, too"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_ZSWG29cEfb"
      },
      "source": [
        "run_name = '5x_i640_b18_nopre_ds' # look for first two in /content/drive/MyDrive/colab_out/\r\n",
        "exp_num = ''                   # leave as '' if just exp\r\n",
        "test_img_size = '1280'           # to what resolution to scale testing images (e.g. 960) must be mult of 32\r\n",
        "test_conf = '0.5'               # confidence threshold for inference testing (e.g. 0.5)\r\n",
        "\r\n",
        "#############################################################################\r\n",
        "\r\n",
        "import os.path\r\n",
        "from os import path\r\n",
        "\r\n",
        "run_path = '/content/drive/MyDrive/colab_out/' + run_name\r\n",
        "weights = run_path + '/weights/best.pt'\r\n",
        "exp_img = path.join('/content/drive/MyDrive/colab_out/exp' + exp_num, '*.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Si8WMHyG-PN"
      },
      "source": [
        "# Evaluate training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIC8gQ8uHLhv"
      },
      "source": [
        "Start tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hFM2Mv3AOYf"
      },
      "source": [
        "%load_ext tensorboard\r\n",
        "%tensorboard --logdir $run_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0HKDUjEIn5h"
      },
      "source": [
        "# Test completed weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3yWg7kKJXPk"
      },
      "source": [
        "Inference speed test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nmZZnWOgJ2S"
      },
      "source": [
        "# when we ran this, we saw .007 second inference time. That is 140 FPS on a TESLA P100!\n",
        "%cd /content/yolov5/\n",
        "!python detect.py --weights $weights --img $test_img_size --conf $test_conf --source /content/drive/MyDrive/my-dataset/test/images --project /content/drive/MyDrive/colab_out/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH9udI2cJp3N"
      },
      "source": [
        "Display inference on all test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "odKEqYtTgbRc"
      },
      "source": [
        "import glob\n",
        "import os.path\n",
        "from os import path\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob(exp_img): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShKsc2g2DxKL"
      },
      "source": [
        "# Issue fixes\r\n",
        "Use only if actually have training issues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rcbww6E9D3FV"
      },
      "source": [
        "# temp fix for image_weights=opt.image_weights error, see: https://github.com/ultralytics/yolov5/issues/1550\r\n",
        "%%writefile /content/yolov5/utils/train.py\r\n",
        "import argparse\r\n",
        "import logging\r\n",
        "import math\r\n",
        "import os\r\n",
        "import random\r\n",
        "import time\r\n",
        "from pathlib import Path\r\n",
        "from warnings import warn\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch.distributed as dist\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "import torch.optim.lr_scheduler as lr_scheduler\r\n",
        "import torch.utils.data\r\n",
        "import yaml\r\n",
        "from torch.cuda import amp\r\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "import test  # import test.py to get mAP after each epoch\r\n",
        "from models.yolo import Model\r\n",
        "from utils.autoanchor import check_anchors\r\n",
        "from utils.datasets import create_dataloader\r\n",
        "from utils.general import labels_to_class_weights, increment_path, labels_to_image_weights, init_seeds, \\\r\n",
        "    fitness, strip_optimizer, get_latest_run, check_dataset, check_file, check_git_status, check_img_size, \\\r\n",
        "    print_mutation, set_logging\r\n",
        "from utils.google_utils import attempt_download\r\n",
        "from utils.loss import compute_loss\r\n",
        "from utils.plots import plot_images, plot_labels, plot_results, plot_evolution\r\n",
        "from utils.torch_utils import ModelEMA, select_device, intersect_dicts, torch_distributed_zero_first\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "\r\n",
        "try:\r\n",
        "    import wandb\r\n",
        "except ImportError:\r\n",
        "    wandb = None\r\n",
        "    logger.info(\"Install Weights & Biases for experiment logging via 'pip install wandb' (recommended)\")\r\n",
        "\r\n",
        "\r\n",
        "def train(hyp, opt, device, tb_writer=None, wandb=None):\r\n",
        "    logger.info(f'Hyperparameters {hyp}')\r\n",
        "    save_dir, epochs, batch_size, total_batch_size, weights, rank = \\\r\n",
        "        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.total_batch_size, opt.weights, opt.global_rank\r\n",
        "\r\n",
        "    # Directories\r\n",
        "    wdir = save_dir / 'weights'\r\n",
        "    wdir.mkdir(parents=True, exist_ok=True)  # make dir\r\n",
        "    last = wdir / 'last.pt'\r\n",
        "    best = wdir / 'best.pt'\r\n",
        "    results_file = save_dir / 'results.txt'\r\n",
        "\r\n",
        "    # Save run settings\r\n",
        "    with open(save_dir / 'hyp.yaml', 'w') as f:\r\n",
        "        yaml.dump(hyp, f, sort_keys=False)\r\n",
        "    with open(save_dir / 'opt.yaml', 'w') as f:\r\n",
        "        yaml.dump(vars(opt), f, sort_keys=False)\r\n",
        "\r\n",
        "    # Configure\r\n",
        "    plots = not opt.evolve  # create plots\r\n",
        "    cuda = device.type != 'cpu'\r\n",
        "    init_seeds(2 + rank)\r\n",
        "    with open(opt.data) as f:\r\n",
        "        data_dict = yaml.load(f, Loader=yaml.FullLoader)  # data dict\r\n",
        "    with torch_distributed_zero_first(rank):\r\n",
        "        check_dataset(data_dict)  # check\r\n",
        "    train_path = data_dict['train']\r\n",
        "    test_path = data_dict['val']\r\n",
        "    nc, names = (1, ['item']) if opt.single_cls else (int(data_dict['nc']), data_dict['names'])  # number classes, names\r\n",
        "    assert len(names) == nc, '%g names found for nc=%g dataset in %s' % (len(names), nc, opt.data)  # check\r\n",
        "\r\n",
        "    # Model\r\n",
        "    pretrained = weights.endswith('.pt')\r\n",
        "    if pretrained:\r\n",
        "        with torch_distributed_zero_first(rank):\r\n",
        "            attempt_download(weights)  # download if not found locally\r\n",
        "        ckpt = torch.load(weights, map_location=device)  # load checkpoint\r\n",
        "        if hyp.get('anchors'):\r\n",
        "            ckpt['model'].yaml['anchors'] = round(hyp['anchors'])  # force autoanchor\r\n",
        "        model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc).to(device)  # create\r\n",
        "        exclude = ['anchor'] if opt.cfg or hyp.get('anchors') else []  # exclude keys\r\n",
        "        state_dict = ckpt['model'].float().state_dict()  # to FP32\r\n",
        "        state_dict = intersect_dicts(state_dict, model.state_dict(), exclude=exclude)  # intersect\r\n",
        "        model.load_state_dict(state_dict, strict=False)  # load\r\n",
        "        logger.info('Transferred %g/%g items from %s' % (len(state_dict), len(model.state_dict()), weights))  # report\r\n",
        "    else:\r\n",
        "        model = Model(opt.cfg, ch=3, nc=nc).to(device)  # create\r\n",
        "\r\n",
        "    # Freeze\r\n",
        "    freeze = []  # parameter names to freeze (full or partial)\r\n",
        "    for k, v in model.named_parameters():\r\n",
        "        v.requires_grad = True  # train all layers\r\n",
        "        if any(x in k for x in freeze):\r\n",
        "            print('freezing %s' % k)\r\n",
        "            v.requires_grad = False\r\n",
        "\r\n",
        "    # Optimizer\r\n",
        "    nbs = 64  # nominal batch size\r\n",
        "    accumulate = max(round(nbs / total_batch_size), 1)  # accumulate loss before optimizing\r\n",
        "    hyp['weight_decay'] *= total_batch_size * accumulate / nbs  # scale weight_decay\r\n",
        "\r\n",
        "    pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\r\n",
        "    for k, v in model.named_modules():\r\n",
        "        if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):\r\n",
        "            pg2.append(v.bias)  # biases\r\n",
        "        if isinstance(v, nn.BatchNorm2d):\r\n",
        "            pg0.append(v.weight)  # no decay\r\n",
        "        elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):\r\n",
        "            pg1.append(v.weight)  # apply decay\r\n",
        "\r\n",
        "    if opt.adam:\r\n",
        "        optimizer = optim.Adam(pg0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\r\n",
        "    else:\r\n",
        "        optimizer = optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\r\n",
        "\r\n",
        "    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\r\n",
        "    optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\r\n",
        "    logger.info('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))\r\n",
        "    del pg0, pg1, pg2\r\n",
        "\r\n",
        "    # Scheduler https://arxiv.org/pdf/1812.01187.pdf\r\n",
        "    # https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR\r\n",
        "    lf = lambda x: ((1 + math.cos(x * math.pi / epochs)) / 2) * (1 - hyp['lrf']) + hyp['lrf']  # cosine\r\n",
        "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\r\n",
        "    # plot_lr_scheduler(optimizer, scheduler, epochs)\r\n",
        "\r\n",
        "    # Logging\r\n",
        "    if wandb and wandb.run is None:\r\n",
        "        opt.hyp = hyp  # add hyperparameters\r\n",
        "        wandb_run = wandb.init(config=opt, resume=\"allow\",\r\n",
        "                               project='YOLOv5' if opt.project == 'runs/train' else Path(opt.project).stem,\r\n",
        "                               name=save_dir.stem,\r\n",
        "                               id=ckpt.get('wandb_id') if 'ckpt' in locals() else None)\r\n",
        "\r\n",
        "    # Resume\r\n",
        "    start_epoch, best_fitness = 0, 0.0\r\n",
        "    if pretrained:\r\n",
        "        # Optimizer\r\n",
        "        if ckpt['optimizer'] is not None:\r\n",
        "            optimizer.load_state_dict(ckpt['optimizer'])\r\n",
        "            best_fitness = ckpt['best_fitness']\r\n",
        "\r\n",
        "        # Results\r\n",
        "        if ckpt.get('training_results') is not None:\r\n",
        "            with open(results_file, 'w') as file:\r\n",
        "                file.write(ckpt['training_results'])  # write results.txt\r\n",
        "\r\n",
        "        # Epochs\r\n",
        "        start_epoch = ckpt['epoch'] + 1\r\n",
        "        if opt.resume:\r\n",
        "            assert start_epoch > 0, '%s training to %g epochs is finished, nothing to resume.' % (weights, epochs)\r\n",
        "        if epochs < start_epoch:\r\n",
        "            logger.info('%s has been trained for %g epochs. Fine-tuning for %g additional epochs.' %\r\n",
        "                        (weights, ckpt['epoch'], epochs))\r\n",
        "            epochs += ckpt['epoch']  # finetune additional epochs\r\n",
        "\r\n",
        "        del ckpt, state_dict\r\n",
        "\r\n",
        "    # Image sizes\r\n",
        "    gs = int(max(model.stride))  # grid size (max stride)\r\n",
        "    imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  # verify imgsz are gs-multiples\r\n",
        "\r\n",
        "    # DP mode\r\n",
        "    if cuda and rank == -1 and torch.cuda.device_count() > 1:\r\n",
        "        model = torch.nn.DataParallel(model)\r\n",
        "\r\n",
        "    # SyncBatchNorm\r\n",
        "    if opt.sync_bn and cuda and rank != -1:\r\n",
        "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\r\n",
        "        logger.info('Using SyncBatchNorm()')\r\n",
        "\r\n",
        "    # EMA\r\n",
        "    ema = ModelEMA(model) if rank in [-1, 0] else None\r\n",
        "\r\n",
        "    # DDP mode\r\n",
        "    if cuda and rank != -1:\r\n",
        "        model = DDP(model, device_ids=[opt.local_rank], output_device=opt.local_rank)\r\n",
        "\r\n",
        "    # Trainloader\r\n",
        "    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, opt,\r\n",
        "                                            hyp=hyp, augment=True, cache=opt.cache_images, rect=opt.rect, rank=rank,\r\n",
        "                                            world_size=opt.world_size, workers=opt.workers)\r\n",
        "    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  # max label class\r\n",
        "    nb = len(dataloader)  # number of batches\r\n",
        "    assert mlc < nc, 'Label class %g exceeds nc=%g in %s. Possible class labels are 0-%g' % (mlc, nc, opt.data, nc - 1)\r\n",
        "\r\n",
        "    # Process 0\r\n",
        "    if rank in [-1, 0]:\r\n",
        "        ema.updates = start_epoch * nb // accumulate  # set EMA updates\r\n",
        "        testloader = create_dataloader(test_path, imgsz_test, total_batch_size, gs, opt,\r\n",
        "                                       hyp=hyp, cache=opt.cache_images and not opt.notest, rect=True,\r\n",
        "                                       rank=-1, world_size=opt.world_size, workers=opt.workers)[0]  # testloader\r\n",
        "\r\n",
        "        if not opt.resume:\r\n",
        "            labels = np.concatenate(dataset.labels, 0)\r\n",
        "            c = torch.tensor(labels[:, 0])  # classes\r\n",
        "            # cf = torch.bincount(c.long(), minlength=nc) + 1.  # frequency\r\n",
        "            # model._initialize_biases(cf.to(device))\r\n",
        "            if plots:\r\n",
        "                plot_labels(labels, save_dir=save_dir)\r\n",
        "                if tb_writer:\r\n",
        "                    tb_writer.add_histogram('classes', c, 0)\r\n",
        "                if wandb:\r\n",
        "                    wandb.log({\"Labels\": [wandb.Image(str(x), caption=x.name) for x in save_dir.glob('*labels*.png')]})\r\n",
        "\r\n",
        "            # Anchors\r\n",
        "            if not opt.noautoanchor:\r\n",
        "                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\r\n",
        "\r\n",
        "    # Model parameters\r\n",
        "    hyp['cls'] *= nc / 80.  # scale coco-tuned hyp['cls'] to current dataset\r\n",
        "    model.nc = nc  # attach number of classes to model\r\n",
        "    model.hyp = hyp  # attach hyperparameters to model\r\n",
        "    model.gr = 1.0  # iou loss ratio (obj_loss = 1.0 or iou)\r\n",
        "    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device)  # attach class weights\r\n",
        "    model.names = names\r\n",
        "\r\n",
        "    # Start training\r\n",
        "    t0 = time.time()\r\n",
        "    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # number of warmup iterations, max(3 epochs, 1k iterations)\r\n",
        "    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training\r\n",
        "    maps = np.zeros(nc)  # mAP per class\r\n",
        "    results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)\r\n",
        "    scheduler.last_epoch = start_epoch - 1  # do not move\r\n",
        "    scaler = amp.GradScaler(enabled=cuda)\r\n",
        "    logger.info('Image sizes %g train, %g test\\n'\r\n",
        "                'Using %g dataloader workers\\nLogging results to %s\\n'\r\n",
        "                'Starting training for %g epochs...' % (imgsz, imgsz_test, dataloader.num_workers, save_dir, epochs))\r\n",
        "    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\r\n",
        "        model.train()\r\n",
        "\r\n",
        "        # Update image weights (optional)\r\n",
        "        if opt.image_weights:\r\n",
        "            # Generate indices\r\n",
        "            if rank in [-1, 0]:\r\n",
        "                cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2  # class weights\r\n",
        "                iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # image weights\r\n",
        "                dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # rand weighted idx\r\n",
        "            # Broadcast if DDP\r\n",
        "            if rank != -1:\r\n",
        "                indices = (torch.tensor(dataset.indices) if rank == 0 else torch.zeros(dataset.n)).int()\r\n",
        "                dist.broadcast(indices, 0)\r\n",
        "                if rank != 0:\r\n",
        "                    dataset.indices = indices.cpu().numpy()\r\n",
        "\r\n",
        "        # Update mosaic border\r\n",
        "        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)\r\n",
        "        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders\r\n",
        "\r\n",
        "        mloss = torch.zeros(4, device=device)  # mean losses\r\n",
        "        if rank != -1:\r\n",
        "            dataloader.sampler.set_epoch(epoch)\r\n",
        "        pbar = enumerate(dataloader)\r\n",
        "        logger.info(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'box', 'obj', 'cls', 'total', 'targets', 'img_size'))\r\n",
        "        if rank in [-1, 0]:\r\n",
        "            pbar = tqdm(pbar, total=nb)  # progress bar\r\n",
        "        optimizer.zero_grad()\r\n",
        "        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\r\n",
        "            ni = i + nb * epoch  # number integrated batches (since train start)\r\n",
        "            imgs = imgs.to(device, non_blocking=True).float() / 255.0  # uint8 to float32, 0-255 to 0.0-1.0\r\n",
        "\r\n",
        "            # Warmup\r\n",
        "            if ni <= nw:\r\n",
        "                xi = [0, nw]  # x interp\r\n",
        "                # model.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)\r\n",
        "                accumulate = max(1, np.interp(ni, xi, [1, nbs / total_batch_size]).round())\r\n",
        "                for j, x in enumerate(optimizer.param_groups):\r\n",
        "                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\r\n",
        "                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\r\n",
        "                    if 'momentum' in x:\r\n",
        "                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\r\n",
        "\r\n",
        "            # Multi-scale\r\n",
        "            if opt.multi_scale:\r\n",
        "                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\r\n",
        "                sf = sz / max(imgs.shape[2:])  # scale factor\r\n",
        "                if sf != 1:\r\n",
        "                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\r\n",
        "                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\r\n",
        "\r\n",
        "            # Forward\r\n",
        "            with amp.autocast(enabled=cuda):\r\n",
        "                pred = model(imgs)  # forward\r\n",
        "                loss, loss_items = compute_loss(pred, targets.to(device), model)  # loss scaled by batch_size\r\n",
        "                if rank != -1:\r\n",
        "                    loss *= opt.world_size  # gradient averaged between devices in DDP mode\r\n",
        "\r\n",
        "            # Backward\r\n",
        "            scaler.scale(loss).backward()\r\n",
        "\r\n",
        "            # Optimize\r\n",
        "            if ni % accumulate == 0:\r\n",
        "                scaler.step(optimizer)  # optimizer.step\r\n",
        "                scaler.update()\r\n",
        "                optimizer.zero_grad()\r\n",
        "                if ema:\r\n",
        "                    ema.update(model)\r\n",
        "\r\n",
        "            # Print\r\n",
        "            if rank in [-1, 0]:\r\n",
        "                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\r\n",
        "                mem = '%.3gG' % (torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\r\n",
        "                s = ('%10s' * 2 + '%10.4g' * 6) % (\r\n",
        "                    '%g/%g' % (epoch, epochs - 1), mem, *mloss, targets.shape[0], imgs.shape[-1])\r\n",
        "                pbar.set_description(s)\r\n",
        "\r\n",
        "                # Plot\r\n",
        "                if plots and ni < 3:\r\n",
        "                    f = save_dir / f'train_batch{ni}.jpg'  # filename\r\n",
        "                    plot_images(images=imgs, targets=targets, paths=paths, fname=f)\r\n",
        "                    # if tb_writer:\r\n",
        "                    #     tb_writer.add_image(f, result, dataformats='HWC', global_step=epoch)\r\n",
        "                    #     tb_writer.add_graph(model, imgs)  # add model to tensorboard\r\n",
        "                elif plots and ni == 3 and wandb:\r\n",
        "                    wandb.log({\"Mosaics\": [wandb.Image(str(x), caption=x.name) for x in save_dir.glob('train*.jpg')]})\r\n",
        "\r\n",
        "            # end batch ------------------------------------------------------------------------------------------------\r\n",
        "        # end epoch ----------------------------------------------------------------------------------------------------\r\n",
        "\r\n",
        "        # Scheduler\r\n",
        "        lr = [x['lr'] for x in optimizer.param_groups]  # for tensorboard\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "        # DDP process 0 or single-GPU\r\n",
        "        if rank in [-1, 0]:\r\n",
        "            # mAP\r\n",
        "            if ema:\r\n",
        "                ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'gr', 'names', 'stride'])\r\n",
        "            final_epoch = epoch + 1 == epochs\r\n",
        "            if not opt.notest or final_epoch:  # Calculate mAP\r\n",
        "                results, maps, times = test.test(opt.data,\r\n",
        "                                                 batch_size=total_batch_size,\r\n",
        "                                                 imgsz=imgsz_test,\r\n",
        "                                                 model=ema.ema,\r\n",
        "                                                 single_cls=opt.single_cls,\r\n",
        "                                                 dataloader=testloader,\r\n",
        "                                                 save_dir=save_dir,\r\n",
        "                                                 plots=plots and final_epoch,\r\n",
        "                                                 log_imgs=opt.log_imgs if wandb else 0)\r\n",
        "\r\n",
        "            # Write\r\n",
        "            with open(results_file, 'a') as f:\r\n",
        "                f.write(s + '%10.4g' * 7 % results + '\\n')  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)\r\n",
        "            if len(opt.name) and opt.bucket:\r\n",
        "                os.system('gsutil cp %s gs://%s/results/results%s.txt' % (results_file, opt.bucket, opt.name))\r\n",
        "\r\n",
        "            # Log\r\n",
        "            tags = ['train/box_loss', 'train/obj_loss', 'train/cls_loss',  # train loss\r\n",
        "                    'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/mAP_0.5:0.95',\r\n",
        "                    'val/box_loss', 'val/obj_loss', 'val/cls_loss',  # val loss\r\n",
        "                    'x/lr0', 'x/lr1', 'x/lr2']  # params\r\n",
        "            for x, tag in zip(list(mloss[:-1]) + list(results) + lr, tags):\r\n",
        "                if tb_writer:\r\n",
        "                    tb_writer.add_scalar(tag, x, epoch)  # tensorboard\r\n",
        "                if wandb:\r\n",
        "                    wandb.log({tag: x})  # W&B\r\n",
        "\r\n",
        "            # Update best mAP\r\n",
        "            fi = fitness(np.array(results).reshape(1, -1))  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]\r\n",
        "            if fi > best_fitness:\r\n",
        "                best_fitness = fi\r\n",
        "\r\n",
        "            # Save model\r\n",
        "            save = (not opt.nosave) or (final_epoch and not opt.evolve)\r\n",
        "            if save:\r\n",
        "                with open(results_file, 'r') as f:  # create checkpoint\r\n",
        "                    ckpt = {'epoch': epoch,\r\n",
        "                            'best_fitness': best_fitness,\r\n",
        "                            'training_results': f.read(),\r\n",
        "                            'model': ema.ema,\r\n",
        "                            'optimizer': None if final_epoch else optimizer.state_dict(),\r\n",
        "                            'wandb_id': wandb_run.id if wandb else None}\r\n",
        "\r\n",
        "                # Save last, best and delete\r\n",
        "                torch.save(ckpt, last)\r\n",
        "                if best_fitness == fi:\r\n",
        "                    torch.save(ckpt, best)\r\n",
        "                del ckpt\r\n",
        "        # end epoch ----------------------------------------------------------------------------------------------------\r\n",
        "    # end training\r\n",
        "\r\n",
        "    if rank in [-1, 0]:\r\n",
        "        # Strip optimizers\r\n",
        "        n = opt.name if opt.name.isnumeric() else ''\r\n",
        "        fresults, flast, fbest = save_dir / f'results{n}.txt', wdir / f'last{n}.pt', wdir / f'best{n}.pt'\r\n",
        "        for f1, f2 in zip([wdir / 'last.pt', wdir / 'best.pt', results_file], [flast, fbest, fresults]):\r\n",
        "            if f1.exists():\r\n",
        "                os.rename(f1, f2)  # rename\r\n",
        "                if str(f2).endswith('.pt'):  # is *.pt\r\n",
        "                    strip_optimizer(f2)  # strip optimizer\r\n",
        "                    os.system('gsutil cp %s gs://%s/weights' % (f2, opt.bucket)) if opt.bucket else None  # upload\r\n",
        "        # Finish\r\n",
        "        if plots:\r\n",
        "            plot_results(save_dir=save_dir)  # save as results.png\r\n",
        "            if wandb:\r\n",
        "                files = ['results.png', 'precision_recall_curve.png', 'confusion_matrix.png']\r\n",
        "                wandb.log({\"Results\": [wandb.Image(str(save_dir / f), caption=f) for f in files\r\n",
        "                                       if (save_dir / f).exists()]})\r\n",
        "        logger.info('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\r\n",
        "    else:\r\n",
        "        dist.destroy_process_group()\r\n",
        "\r\n",
        "    wandb.run.finish() if wandb and wandb.run else None\r\n",
        "    torch.cuda.empty_cache()\r\n",
        "    return results\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    parser = argparse.ArgumentParser()\r\n",
        "    parser.add_argument('--weights', type=str, default='yolov5s.pt', help='initial weights path')\r\n",
        "    parser.add_argument('--cfg', type=str, default='', help='model.yaml path')\r\n",
        "    parser.add_argument('--data', type=str, default='data/coco128.yaml', help='data.yaml path')\r\n",
        "    parser.add_argument('--hyp', type=str, default='data/hyp.scratch.yaml', help='hyperparameters path')\r\n",
        "    parser.add_argument('--epochs', type=int, default=300)\r\n",
        "    parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs')\r\n",
        "    parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='[train, test] image sizes')\r\n",
        "    parser.add_argument('--rect', action='store_true', help='rectangular training')\r\n",
        "    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')\r\n",
        "    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')\r\n",
        "    parser.add_argument('--notest', action='store_true', help='only test final epoch')\r\n",
        "    parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')\r\n",
        "    parser.add_argument('--evolve', action='store_true', help='evolve hyperparameters')\r\n",
        "    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')\r\n",
        "    parser.add_argument('--cache-images', action='store_true', help='cache images for faster training')\r\n",
        "    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')\r\n",
        "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\r\n",
        "    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')\r\n",
        "    parser.add_argument('--single-cls', action='store_true', help='train as single-class dataset')\r\n",
        "    parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer')\r\n",
        "    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')\r\n",
        "    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')\r\n",
        "    parser.add_argument('--log-imgs', type=int, default=16, help='number of images for W&B logging, max 100')\r\n",
        "    parser.add_argument('--workers', type=int, default=8, help='maximum number of dataloader workers')\r\n",
        "    parser.add_argument('--project', default='runs/train', help='save to project/name')\r\n",
        "    parser.add_argument('--name', default='exp', help='save to project/name')\r\n",
        "    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')\r\n",
        "    opt = parser.parse_args()\r\n",
        "\r\n",
        "    # Set DDP variables\r\n",
        "    opt.total_batch_size = opt.batch_size\r\n",
        "    opt.world_size = int(os.environ['WORLD_SIZE']) if 'WORLD_SIZE' in os.environ else 1\r\n",
        "    opt.global_rank = int(os.environ['RANK']) if 'RANK' in os.environ else -1\r\n",
        "    set_logging(opt.global_rank)\r\n",
        "    if opt.global_rank in [-1, 0]:\r\n",
        "        check_git_status()\r\n",
        "\r\n",
        "    # Resume\r\n",
        "    if opt.resume:  # resume an interrupted run\r\n",
        "        ckpt = opt.resume if isinstance(opt.resume, str) else get_latest_run()  # specified or most recent path\r\n",
        "        assert os.path.isfile(ckpt), 'ERROR: --resume checkpoint does not exist'\r\n",
        "        with open(Path(ckpt).parent.parent / 'opt.yaml') as f:\r\n",
        "            opt = argparse.Namespace(**yaml.load(f, Loader=yaml.FullLoader))  # replace\r\n",
        "        opt.cfg, opt.weights, opt.resume = '', ckpt, True\r\n",
        "        logger.info('Resuming training from %s' % ckpt)\r\n",
        "    else:\r\n",
        "        # opt.hyp = opt.hyp or ('hyp.finetune.yaml' if opt.weights else 'hyp.scratch.yaml')\r\n",
        "        opt.data, opt.cfg, opt.hyp = check_file(opt.data), check_file(opt.cfg), check_file(opt.hyp)  # check files\r\n",
        "        assert len(opt.cfg) or len(opt.weights), 'either --cfg or --weights must be specified'\r\n",
        "        opt.img_size.extend([opt.img_size[-1]] * (2 - len(opt.img_size)))  # extend to 2 sizes (train, test)\r\n",
        "        opt.name = 'evolve' if opt.evolve else opt.name\r\n",
        "        opt.save_dir = increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok | opt.evolve)  # increment run\r\n",
        "\r\n",
        "    # DDP mode\r\n",
        "    device = select_device(opt.device, batch_size=opt.batch_size)\r\n",
        "    if opt.local_rank != -1:\r\n",
        "        assert torch.cuda.device_count() > opt.local_rank\r\n",
        "        torch.cuda.set_device(opt.local_rank)\r\n",
        "        device = torch.device('cuda', opt.local_rank)\r\n",
        "        dist.init_process_group(backend='nccl', init_method='env://')  # distributed backend\r\n",
        "        assert opt.batch_size % opt.world_size == 0, '--batch-size must be multiple of CUDA device count'\r\n",
        "        opt.batch_size = opt.total_batch_size // opt.world_size\r\n",
        "\r\n",
        "    # Hyperparameters\r\n",
        "    with open(opt.hyp) as f:\r\n",
        "        hyp = yaml.load(f, Loader=yaml.FullLoader)  # load hyps\r\n",
        "        if 'box' not in hyp:\r\n",
        "            warn('Compatibility: %s missing \"box\" which was renamed from \"giou\" in %s' %\r\n",
        "                 (opt.hyp, 'https://github.com/ultralytics/yolov5/pull/1120'))\r\n",
        "            hyp['box'] = hyp.pop('giou')\r\n",
        "\r\n",
        "    # Train\r\n",
        "    logger.info(opt)\r\n",
        "    if not opt.evolve:\r\n",
        "        tb_writer = None  # init loggers\r\n",
        "        if opt.global_rank in [-1, 0]:\r\n",
        "            logger.info(f'Start Tensorboard with \"tensorboard --logdir {opt.project}\", view at http://localhost:6006/')\r\n",
        "            tb_writer = SummaryWriter(opt.save_dir)  # Tensorboard\r\n",
        "        train(hyp, opt, device, tb_writer, wandb)\r\n",
        "\r\n",
        "    # Evolve hyperparameters (optional)\r\n",
        "    else:\r\n",
        "        # Hyperparameter evolution metadata (mutation scale 0-1, lower_limit, upper_limit)\r\n",
        "        meta = {'lr0': (1, 1e-5, 1e-1),  # initial learning rate (SGD=1E-2, Adam=1E-3)\r\n",
        "                'lrf': (1, 0.01, 1.0),  # final OneCycleLR learning rate (lr0 * lrf)\r\n",
        "                'momentum': (0.3, 0.6, 0.98),  # SGD momentum/Adam beta1\r\n",
        "                'weight_decay': (1, 0.0, 0.001),  # optimizer weight decay\r\n",
        "                'warmup_epochs': (1, 0.0, 5.0),  # warmup epochs (fractions ok)\r\n",
        "                'warmup_momentum': (1, 0.0, 0.95),  # warmup initial momentum\r\n",
        "                'warmup_bias_lr': (1, 0.0, 0.2),  # warmup initial bias lr\r\n",
        "                'box': (1, 0.02, 0.2),  # box loss gain\r\n",
        "                'cls': (1, 0.2, 4.0),  # cls loss gain\r\n",
        "                'cls_pw': (1, 0.5, 2.0),  # cls BCELoss positive_weight\r\n",
        "                'obj': (1, 0.2, 4.0),  # obj loss gain (scale with pixels)\r\n",
        "                'obj_pw': (1, 0.5, 2.0),  # obj BCELoss positive_weight\r\n",
        "                'iou_t': (0, 0.1, 0.7),  # IoU training threshold\r\n",
        "                'anchor_t': (1, 2.0, 8.0),  # anchor-multiple threshold\r\n",
        "                'anchors': (2, 2.0, 10.0),  # anchors per output grid (0 to ignore)\r\n",
        "                'fl_gamma': (0, 0.0, 2.0),  # focal loss gamma (efficientDet default gamma=1.5)\r\n",
        "                'hsv_h': (1, 0.0, 0.1),  # image HSV-Hue augmentation (fraction)\r\n",
        "                'hsv_s': (1, 0.0, 0.9),  # image HSV-Saturation augmentation (fraction)\r\n",
        "                'hsv_v': (1, 0.0, 0.9),  # image HSV-Value augmentation (fraction)\r\n",
        "                'degrees': (1, 0.0, 45.0),  # image rotation (+/- deg)\r\n",
        "                'translate': (1, 0.0, 0.9),  # image translation (+/- fraction)\r\n",
        "                'scale': (1, 0.0, 0.9),  # image scale (+/- gain)\r\n",
        "                'shear': (1, 0.0, 10.0),  # image shear (+/- deg)\r\n",
        "                'perspective': (0, 0.0, 0.001),  # image perspective (+/- fraction), range 0-0.001\r\n",
        "                'flipud': (1, 0.0, 1.0),  # image flip up-down (probability)\r\n",
        "                'fliplr': (0, 0.0, 1.0),  # image flip left-right (probability)\r\n",
        "                'mosaic': (1, 0.0, 1.0),  # image mixup (probability)\r\n",
        "                'mixup': (1, 0.0, 1.0)}  # image mixup (probability)\r\n",
        "\r\n",
        "        assert opt.local_rank == -1, 'DDP mode not implemented for --evolve'\r\n",
        "        opt.notest, opt.nosave = True, True  # only test/save final epoch\r\n",
        "        # ei = [isinstance(x, (int, float)) for x in hyp.values()]  # evolvable indices\r\n",
        "        yaml_file = Path(opt.save_dir) / 'hyp_evolved.yaml'  # save best result here\r\n",
        "        if opt.bucket:\r\n",
        "            os.system('gsutil cp gs://%s/evolve.txt .' % opt.bucket)  # download evolve.txt if exists\r\n",
        "\r\n",
        "        for _ in range(300):  # generations to evolve\r\n",
        "            if Path('evolve.txt').exists():  # if evolve.txt exists: select best hyps and mutate\r\n",
        "                # Select parent(s)\r\n",
        "                parent = 'single'  # parent selection method: 'single' or 'weighted'\r\n",
        "                x = np.loadtxt('evolve.txt', ndmin=2)\r\n",
        "                n = min(5, len(x))  # number of previous results to consider\r\n",
        "                x = x[np.argsort(-fitness(x))][:n]  # top n mutations\r\n",
        "                w = fitness(x) - fitness(x).min()  # weights\r\n",
        "                if parent == 'single' or len(x) == 1:\r\n",
        "                    # x = x[random.randint(0, n - 1)]  # random selection\r\n",
        "                    x = x[random.choices(range(n), weights=w)[0]]  # weighted selection\r\n",
        "                elif parent == 'weighted':\r\n",
        "                    x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination\r\n",
        "\r\n",
        "                # Mutate\r\n",
        "                mp, s = 0.8, 0.2  # mutation probability, sigma\r\n",
        "                npr = np.random\r\n",
        "                npr.seed(int(time.time()))\r\n",
        "                g = np.array([x[0] for x in meta.values()])  # gains 0-1\r\n",
        "                ng = len(meta)\r\n",
        "                v = np.ones(ng)\r\n",
        "                while all(v == 1):  # mutate until a change occurs (prevent duplicates)\r\n",
        "                    v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)\r\n",
        "                for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)\r\n",
        "                    hyp[k] = float(x[i + 7] * v[i])  # mutate\r\n",
        "\r\n",
        "            # Constrain to limits\r\n",
        "            for k, v in meta.items():\r\n",
        "                hyp[k] = max(hyp[k], v[1])  # lower limit\r\n",
        "                hyp[k] = min(hyp[k], v[2])  # upper limit\r\n",
        "                hyp[k] = round(hyp[k], 5)  # significant digits\r\n",
        "\r\n",
        "            # Train mutation\r\n",
        "            results = train(hyp.copy(), opt, device, wandb=wandb)\r\n",
        "\r\n",
        "            # Write mutation results\r\n",
        "            print_mutation(hyp.copy(), results, yaml_file, opt.bucket)\r\n",
        "\r\n",
        "        # Plot results\r\n",
        "        plot_evolution(yaml_file)\r\n",
        "        print(f'Hyperparameter evolution complete. Best results saved as: {yaml_file}\\n'\r\n",
        "              f'Command to train a new model with these hyperparameters: $ python train.py --hyp {yaml_file}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}